# -*- coding: utf-8 -*-
"""Homework 3

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TPk0uiVyZVWPYQPuGyQ-OMqcrel3LSmm
"""

import os
from pandas import read_csv
from numpy import set_printoptions
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import Normalizer
from sklearn.preprocessing import Binarizer
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
from sklearn.decomposition import PCA
from sklearn.ensemble import ExtraTreesClassifier
import pandas as pd

os.chdir('/content/drive/MyDrive/data ML')

"""Раздел 1. Предварительная обработка данных
1.	Укажите путь к рабочей папке, где хранятся данные, с помощью функции chdir() из модуля os.
2.	Загрузите данные файла pima-indians-diabetes.data.csv с помощью функции read_csv() из модуля pandas. Посмотрите на первые 5 наблюдений с помощью функции head().





3.	Сохраните в переменную array значения признаков, обратившись к атрибуту values.
4.	Выделите в переменной array часть, которая относится к факторным признакам, назовите ее X, и часть, которая относится к результативному признаку, - Y. Прокомментируйте содержимое переменных X и Y.



5.	Проведите масштабирование факторных признаков на отрезок [0,1]. Для этого сначала создайте соответствующий объект с помощью функции MinMaxScaler() из модуля sklearn.preprocessing. Потом обучите его на факторных признаках и преобразуйте данные с помощью функции fit_transform().
6.	Установите формат представления данных вида 3 знака после запятой, воспользовавшись функцией set_printoptions() из модуля numpy.
7.	Выведите на экран первые 5 строк преобразованных данных.

"""

data=read_csv('/content/drive/MyDrive/data ML/country-wise-average.csv')
data.head()

data.info()

data.describe()

df = data.dropna()

array=df.values

X=array[:,3:8]
Y=array[:,2]

Y=Y.astype('int')

scaler=MinMaxScaler(feature_range=(0,1))

rescaledX=scaler.fit_transform(X)

set_printoptions(precision=3)

print(rescaledX[0:5,:])

"""8. Проведите стандартизацию факторных признаков. Для этого сначала создайте соответствующий объект с помощью функции StandardScaler() из модуля sklearn.preprocessing. Потом обучите его на факторных признаках с помощью функции fit() и преобразуйте данные с помощью функции transform(). Выведите на экран первые 5 строк преобразованных данных"""

scaler=StandardScaler().fit(X)

rescaledX=scaler.transform(X)

data.head(5)

"""9.9.	Проведите нормализацию факторных признаков. Для этого сначала создайте соответствующий объект с помощью функции Normalizer() из модуля sklearn.preprocessing. Потом обучите его на факторных признаках с помощью функции fit() и преобразуйте данные с помощью функции transform(). Выведите на экран первые 5 строк преобразованных данных."""

scaler=Normalizer().fit(X)

rescaledX=scaler.transform(X)

print(rescaledX[0:5,:])

"""10.	Проведите бинарное кодирование факторных признаков так, чтобы значение признака, большее 0, стало равным 1. Для этого сначала создайте соответствующий объект с помощью функции Binarizer(threshold=0.0) из модуля sklearn.preprocessing. Потом обучите его на факторных признаках с помощью функции fit() и преобразуйте данные с помощью функции transform(). Выведите на экран первые 5 строк преобразованных данных."""

bin=Binarizer(threshold=0.0).fit(X)

binaryX=bin.transform(X)

print(binaryX[0:5,:])

"""11.	Проведите отбор 4 наиболее значимых признаков на основе F-критерия в дисперсионном анализе. Для этого сначала создайте соответствующий объект с помощью функции SelectKBest(score_func=f_classif, k=4) из модуля sklearn.feature_selection. Потом обучите его на переменных X и Y с помощью функции fit(). Выведите на экран значения F-критерия для всех признаков, обратившись к параметру scores_."""

test=SelectKBest(score_func=f_classif, k=4)

fit=test.fit(X,Y)

print(fit.scores_)

"""
12.	Преобразуйте факторные признаки с помощью функции transform(). Выведите на экран первые 5 строк преобразованных данных.
"""

features=fit.transform(X)

print(features[0:5,:])

"""13.	Проводите рекурсивное исключение наименее значимых признаков до тех пор, пока не останется 3 признака. Отбор осуществляйте на основе модели логистической регрессии. Сначала создайте модель с помощью функции LogisticRegression() из модуля sklearn.linear_model. """

model=LogisticRegression(solver='liblinear')

"""14.	Затем с помощью функции RFE() из модуля sklearn.feature_selection создайте объект для рекурсивного исключения признаков, подав ему на вход созданную модель логистической регрессии и количество отбираемых признаков (n_features_to_select=3). """

rfe=RFE(model,n_features_to_select=3)

"""15.	Обучите созданный объект на переменных X и Y с помощью функции fit(). """

fit=rfe.fit(X,Y)

"""16.	Выведите на экран число отобранных признаков, обратившись к параметру n_features_."""

fit.n_features_

"""17.	Выведите на экран логическое указание на отобранные признаки из всего набора, обратившись к параметру support_.

"""

fit.support_

"""18.	Выведите на экран ранги признаков, обратившись к параметру ranking_.


"""

fit.ranking_

"""19.	Преобразуйте факторные признаки с помощью функции fit_transform(). Выведите на экран первые 5 строк преобразованных данных."""

tr = rfe.fit_transform(X,Y)
print(tr[0:5,:])

"""20.	Проведите отбор 3-х главных компонент. Для этого сначала создайте соответствующий объект с помощью функции PCA(n_components=3) из модуля sklearn.decomposition. """

pca=PCA(n_components=3)

"""21.	Обучите его на факторных признаках с помощью функции fit()."""

fit=pca.fit(X)

"""22.	Выведите на экран долю объясненной 3-мя компонентами дисперсии, обратившись к параметру explained_variance_ratio_."""

fit.explained_variance_ratio_

"""23.	Выведите на экран компонентные нагрузки, обратившись к параметру components_. Что они показывают?"""

fit.components_

"""24.	Вычислите на основе факторных признаков главные компоненты, обратившись к функции fit_transform(). Выведите на экран первые 5 наблюдений."""

tr1 = pca.fit_transform(X)
print(tr1[0:5,:])

"""25.	Примените классификатор дополнительных деревьев для отбора признаков, воспользовавшись встроенным в них механизмом расчета критериев информативности при поиске, по какому признаку осуществлять разбиение. Для этого сначала создайте соответствующий объект с помощью функции ExtraTreesClassifier(n_estimators=100) из модуля sklearn.ensemble.
26.	Обучите его на переменных X и Y с помощью функции fit().

"""

model=ExtraTreesClassifier(n_estimators=100)

model.fit(X,Y)

"""27. Выведите на экран значения критерия информативности для каждого признака, обратившись к параметру feature_importances_."""

print(model.feature_importances_)